Simple Docker Setup for running Local LLM using OpenWebUI as frontend and Llama.cpp (with CUDA) as backend. Using Caddy for Local domain routing.
ComfyUI is included for image generation.
